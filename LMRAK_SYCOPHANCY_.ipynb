{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F89EaELcAAMb"
      },
      "outputs": [],
      "source": [
        "# Install PyTorch, Hugging Face Transformers, and other libraries\n",
        "!pip install torch==2.0.1\n",
        "!pip install transformers==4.31.0\n",
        "!pip install datasets\n",
        "!pip install matplotlib seaborn\n",
        "!pip install bertviz\n",
        "!pip install git+https://github.com/pytorch/captum.git  # for advanced interpretability\n",
        "\n",
        "# Optionally, if you want to clone path patching or other relevant repos:\n",
        "# !git clone https://github.com/your-username/path-patching-repo.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "\n",
        "# For interpretability/visualization\n",
        "from bertviz import head_view\n",
        "from captum.attr import LayerIntegratedGradients"
      ],
      "metadata": {
        "id": "WXOfQ_wIPe5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade datasets"
      ],
      "metadata": {
        "id": "DNVyGuSWPiVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "id": "nMsykarMPljh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "CvW-U-8HPo-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the TruthfulQA dataset (choose the appropriate configuration)\n",
        "# Option 1: 'generation' configuration for free-text generation tasks\n",
        "truthfulqa_generation = load_dataset(\"truthful_qa\", \"generation\", split=\"validation\")\n",
        "\n",
        "# Option 2: 'multiple_choice' configuration for multiple-choice tasks\n",
        "truthfulqa_multiple_choice = load_dataset(\"truthful_qa\", \"multiple_choice\", split=\"validation\")\n",
        "\n",
        "# Inspect the dataset structure\n",
        "print(\"TruthfulQA (Generation) Example:\")\n",
        "print(truthfulqa_generation[0])  # First example in the 'generation' config\n",
        "\n",
        "print(\"\\nTruthfulQA (Multiple Choice) Example:\")\n",
        "print(truthfulqa_multiple_choice[0])  # First example in the 'multiple_choice' config"
      ],
      "metadata": {
        "id": "Uooh-MlQPsd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example hooking function for a single layer\n",
        "from torch.nn import Module\n",
        "\n",
        "def patch_activation(module: Module, input, output):\n",
        "    \"\"\"\n",
        "    This function can modify 'output' in real time.\n",
        "    For path patching, you might replace or scale certain channels\n",
        "    in the hidden representation.\n",
        "    \"\"\"\n",
        "    # Example: zero out a certain dimension (just a placeholder, adapt for your research)\n",
        "    # output[:, :, some_dimension] = 0\n",
        "    return output\n",
        "\n",
        "# Register hooks for a specific layer, e.g., transformer.h.1\n",
        "layer_to_patch = model.transformer.h[1]  # Just as an example\n",
        "hook_handle = layer_to_patch.register_forward_hook(patch_activation)\n",
        "\n",
        "# Test forward pass\n",
        "test_text = \"Hello, how are you?\"\n",
        "inputs = tokenizer(test_text, return_tensors='pt').to(device)\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "hook_handle.remove()  # remove the hook after testin"
      ],
      "metadata": {
        "id": "ZJUfKlXiPv_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example code snippet for capturing hidden states\n",
        "from collections import defaultdict\n",
        "\n",
        "def capture_activations(model, text):\n",
        "    activations_dict = defaultdict(list)\n",
        "\n",
        "    def get_hook(layer_name):\n",
        "        def hook_fn(module, input, output):\n",
        "            activations_dict[layer_name].append(output.detach().cpu())\n",
        "        return hook_fn\n",
        "\n",
        "    # Register hooks\n",
        "    handles = []\n",
        "    for i, block in enumerate(model.transformer.h):\n",
        "        layer_name = f\"layer_{i}\"\n",
        "        handles.append(block.register_forward_hook(get_hook(layer_name)))\n",
        "\n",
        "    # Forward pass\n",
        "    inputs = tokenizer(text, return_tensors='pt').to(device)\n",
        "    with torch.no_grad():\n",
        "        _ = model(**inputs)\n",
        "\n",
        "    # Remove hooks\n",
        "    for handle in handles:\n",
        "        handle.remove()\n",
        "\n",
        "    return activations_dict\n",
        "\n",
        "# Example usage:\n",
        "sycophantic_text = \"The Earth is definitely flat, right?\"\n",
        "reasoning_text = \"Scientific evidence shows the Earth is round.\"\n",
        "\n",
        "sycophantic_acts = capture_activations(model, sycophantic_text)\n",
        "reasoning_acts = capture_activations(model, reasoning_text)\n",
        "\n",
        "# Compute a \"task vector\" for layer_1 as an example\n",
        "task_vector_layer_1 = reasoning_acts['layer_1'][0] - sycophantic_acts['layer_1'][0]"
      ],
      "metadata": {
        "id": "FOsKKVYsPzY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of extracting and visualizing attention\n",
        "model_config = model.config\n",
        "model_config.output_attentions = True\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name, config=model_config).to(device)\n",
        "\n",
        "inputs = tokenizer(\"Why do people say the earth is flat?\", return_tensors='pt').to(device)\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    # outputs.attentions is a tuple of attention weights from each layer\n",
        "attentions = outputs.attentions  # shape: (num_layers, batch_size, num_heads, seq_len, seq_len)\n",
        "\n",
        "# Using BertViz's head_view\n",
        "sentence = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "_ = head_view(attentions, sentence, sentence)"
      ],
      "metadata": {
        "id": "5DUER5XVPsfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze all parameters\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Unfreeze only the identified heads/layers\n",
        "# For GPT-2, the attention heads are under `model.transformer.h[i].attn`\n",
        "layer_index = 10\n",
        "head_indices = [2, 5]\n",
        "\n",
        "# GPT-2 uses a single multi-head projection. One approach is partial-fine-tuning that projection:\n",
        "for name, param in model.transformer.h[layer_index].attn.c_attn.named_parameters():\n",
        "    # You could refine further by indexing param slices corresponding to heads\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Alternatively, you might do something more advanced with partial slices of the weight matrix."
      ],
      "metadata": {
        "id": "QshtoWG4P7pI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple training loop for demonstration\n",
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5)\n",
        "\n",
        "train_texts = [\n",
        "    \"The Earth is flat, correct?\",\n",
        "    \"Vaccines cause autism, right?\",\n",
        "    # ... (you could load from a proper dataset)\n",
        "]\n",
        "labels = [\n",
        "    \"No, the Earth is round.\",\n",
        "    \"No, scientific consensus finds no link.\",\n",
        "    # ...\n",
        "]\n",
        "\n",
        "# Convert to a form suitable for training\n",
        "def tokenize_batch(batch_texts):\n",
        "    return tokenizer(batch_texts, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "model.train()\n",
        "for epoch in range(1):  # Simplified\n",
        "    for text, label in zip(train_texts, labels):\n",
        "        inputs = tokenizer(text, return_tensors='pt').to(device)\n",
        "        labels_t = tokenizer(label, return_tensors='pt')['input_ids'].to(device)\n",
        "\n",
        "        # Shift labels by 1 for language modeling\n",
        "        outputs = model(**inputs, labels=labels_t)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        print(f\"Loss: {loss.item()}\")"
      ],
      "metadata": {
        "id": "JqPM-cAIP97X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard fine-tuning approach, unfreeze everything\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
        "# Then follow a similar training loop as above, but now with all parameters trainable."
      ],
      "metadata": {
        "id": "5pHapf6tP_gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "def evaluate_model(model, dataset):\n",
        "    # Evaluate synergy between correctness and reduced sycophancy\n",
        "    # Return relevant metric(s)\n",
        "    pass\n",
        "\n",
        "improved_performance = evaluate_model(model, truthfulqa)\n",
        "print(\"Pinpoint-tuned performance on sycophancy tasks:\", improved_performance)"
      ],
      "metadata": {
        "id": "G3SgtIWpP_iO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recompute activations for sycophantic vs. reasoning text with the newly tuned model\n",
        "syc_acts_new = capture_activations(model, sycophantic_text)\n",
        "reason_acts_new = capture_activations(model, reasoning_text)\n",
        "\n",
        "# Example: compute difference for layer_1\n",
        "task_vector_layer_1_new = reason_acts_new['layer_1'][0] - syc_acts_new['layer_1'][0]\n",
        "\n",
        "# This vector can be stored or applied to other models' activations"
      ],
      "metadata": {
        "id": "PL9UYF_cQEK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_task_vector(activation, task_vector, alpha=1.0):\n",
        "    return activation + alpha * task_vector\n",
        "\n",
        "# In a forward hook for a second model:\n",
        "def patch_with_task_vector(module, input, output):\n",
        "    # Suppose we've identified layer_1 as target\n",
        "    return apply_task_vector(output, task_vector_layer_1_new, alpha=0.5)"
      ],
      "metadata": {
        "id": "uuQhtRdKQFwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_baseline(model, dataset, tokenizer):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for example in dataset:\n",
        "        prompt = example['question']  # Replace with dataset's structure\n",
        "        correct_answer = example['answer']\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(**inputs, max_new_tokens=50)\n",
        "            generated_answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        if correct_answer.lower() in generated_answer.lower():\n",
        "            correct += 1\n",
        "        total += 1\n",
        "    return correct / total\n",
        "\n",
        "baseline_accuracy = evaluate_baseline(model, truthfulqa, tokenizer)\n",
        "print(\"Baseline Accuracy:\", baseline_accuracy)"
      ],
      "metadata": {
        "id": "ev0yB7ZvQHmt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}